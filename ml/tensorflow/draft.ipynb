{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正则化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(14.03604, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.random.normal((4, 3))\n",
    "w2 = tf.random.normal((4, 2))\n",
    "\n",
    "loss_reg = tf.reduce_sum(tf.abs(w1)) + tf.reduce_sum(tf.abs(w2))\n",
    "print(loss_reg) # 正则化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 卷积运算\n",
    "  \n",
    "  感受野与卷积核**逐位相乘**后累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1:  (2, 3, 3, 4)\n",
      "out2:  (2, 5, 5, 4)\n",
      "out3:  (2, 3, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# 输入格式为：batch_shape + [in_height, in_width, in_channels]\n",
    "x = tf.random.normal([2, 5, 5, 3]) # 模拟输入，3通道，高宽为5\n",
    "# 创建4个3x3的卷积核，格式为[filter_height, filter_width, in_channels, out_channels]\n",
    "w = tf.random.normal([3, 3, 3, 4]) \n",
    "\n",
    "## 1. padding为0，步长为1\n",
    "out1 = tf.nn.conv2d(input=x, filters=w, strides=1, padding=[[0, 0], [0, 0], [0, 0], [0, 0]])\n",
    "print(\"out1: \", out1.shape)\n",
    "\n",
    "## 2. 上下左右各填充1个单位，步长为1\n",
    "#    when data_format is `\"NHWC\"`, padding should be in the form \n",
    "#           `[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]`. \n",
    "#    When explicit padding used and data_format is `\"NCHW\"`, this should be in the form \n",
    "#           `[[0, 0], [0, 0], [pad_top, pad_bottom], [pad_left, pad_right]]`.\n",
    "out2 = tf.nn.conv2d(input=x, filters=w, strides=1, padding=[[0, 0], [1, 1], [1, 1], [0, 0]])\n",
    "print(\"out2: \", out2.shape)\n",
    "\n",
    "## 3. padding=\"SAME\", 输出维度的高宽成1/s倍减少\n",
    "out3 = tf.nn.conv2d(input=x, filters=w, strides=2, padding=\"SAME\")\n",
    "print(\"out3: \", out3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 卷积层类\n",
    "\n",
    "`Conv2D`: 卷积层类，里面保存了张量W和偏置b，可以通过`trainable_variables`、`kernel`、`bias`等成员获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 创建4个3x3大小的卷积核层\n",
    "layer1 = tf.keras.layers.Conv2D(4, kernel_size=3, strides=1, padding=\"SAME\")\n",
    "\n",
    "## 2. 创建4个高x宽为3x4大小的卷积核，高宽方向的步长为2和1\n",
    "layer2 = tf.keras.layers.Conv2D(4, kernel_size=(3, 4), strides=(2, 1), padding=\"SAME\")\n",
    "\n",
    "out = layer1(x) # 前向计算\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 池化层\n",
    "\n",
    "从局部相关的一组元素中采样或进行信息聚合，从而得到新的元素值\n",
    "\n",
    "最大池化（Max Pooling）：最大值\n",
    "平均池化（Average Pooling）：平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BatchNormal层\n",
    "  \n",
    "  测试模式与训练模式要区分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "# 2 images with 4x4 size, 3 channels\n",
    "x = tf.random.normal([2, 4, 4, 3], mean=.1, stddev=0.5)\n",
    "\n",
    "net = layers.BatchNormalization(axis=-1, center=True, scale=True, trainable=True)\n",
    "\n",
    "out = net(x)\n",
    "print(\"forward in test model: \", net.variables)\n",
    "\n",
    "out = net(x, training=True)\n",
    "print(\"forward in train model(1 step): \", net.variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    out = net(x, training=True)\n",
    "print('forward in train mode(100 steps):', net.variables)\n",
    "\n",
    "\n",
    "optimizer = optimizers.SGD(lr=1e-2)\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = net(x, training=True)\n",
    "        loss = tf.reduce_mean(tf.pow(out, 2)) - 1\n",
    "\n",
    "    grads = tape.gradient(loss, net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "print('backward(10 steps):', net.variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CIFAR10训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 11:13:26.091663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/yeting/.mujoco/mujoco210/bin\n",
      "2022-07-22 11:13:26.091740: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-22 11:13:26.091819: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: eb23ed7a4bcf\n",
      "2022-07-22 11:13:26.091847: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: eb23ed7a4bcf\n",
      "2022-07-22 11:13:26.091999: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2022-07-22 11:13:26.092106: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.80.2\n",
      "2022-07-22 11:13:26.098730: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, optimizers, datasets, Sequential\n",
    "import os \n",
    "\n",
    "(x, y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    # [0, 1]\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 4, 4, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 2, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,404,992\n",
      "Trainable params: 9,404,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165,514\n",
      "Trainable params: 165,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 0 loss:  2.30195951461792\n",
      "0 100 loss:  1.847825288772583\n",
      "0 200 loss:  1.6494622230529785\n",
      "0 300 loss:  1.5875260829925537\n",
      "0  acc:  0.4647\n",
      "1 0 loss:  1.384695291519165\n",
      "1 100 loss:  1.4833617210388184\n",
      "1 200 loss:  1.36872398853302\n",
      "1 300 loss:  1.2328388690948486\n",
      "1  acc:  0.5292\n",
      "2 0 loss:  1.322127103805542\n",
      "2 100 loss:  1.252114176750183\n",
      "2 200 loss:  1.1731600761413574\n",
      "2 300 loss:  1.1051064729690552\n",
      "2  acc:  0.6062\n",
      "3 0 loss:  1.0995482206344604\n",
      "3 100 loss:  1.1514110565185547\n",
      "3 200 loss:  1.055976390838623\n",
      "3 300 loss:  0.7576755881309509\n",
      "3  acc:  0.6566\n",
      "4 0 loss:  0.9433736801147461\n",
      "4 100 loss:  0.9222942590713501\n",
      "4 200 loss:  0.8802751302719116\n",
      "4 300 loss:  0.9273163676261902\n",
      "4  acc:  0.7029\n",
      "5 0 loss:  0.6636196374893188\n",
      "5 100 loss:  0.8354804515838623\n",
      "5 200 loss:  0.7601966857910156\n",
      "5 300 loss:  0.4955000579357147\n",
      "5  acc:  0.7109\n",
      "6 0 loss:  0.6298633813858032\n",
      "6 100 loss:  0.7796262502670288\n",
      "6 200 loss:  0.7237041592597961\n",
      "6 300 loss:  0.5713860988616943\n",
      "6  acc:  0.7295\n",
      "7 0 loss:  0.4752826988697052\n",
      "7 100 loss:  0.5214059352874756\n",
      "7 200 loss:  0.5108321905136108\n",
      "7 300 loss:  0.41196781396865845\n",
      "7  acc:  0.7573\n",
      "8 0 loss:  0.5703933238983154\n",
      "8 100 loss:  0.4956623315811157\n",
      "8 200 loss:  0.39801710844039917\n",
      "8 300 loss:  0.42127472162246704\n",
      "8  acc:  0.763\n",
      "9 0 loss:  0.3262398838996887\n",
      "9 100 loss:  0.35588380694389343\n",
      "9 200 loss:  0.36190110445022583\n",
      "9 300 loss:  0.2001565396785736\n",
      "9  acc:  0.7598\n",
      "10 0 loss:  0.2444315105676651\n",
      "10 100 loss:  0.21718314290046692\n",
      "10 200 loss:  0.28592416644096375\n",
      "10 300 loss:  0.2135736346244812\n",
      "10  acc:  0.7669\n",
      "11 0 loss:  0.21864917874336243\n",
      "11 100 loss:  0.18280725181102753\n",
      "11 200 loss:  0.13084422051906586\n",
      "11 300 loss:  0.13899119198322296\n",
      "11  acc:  0.766\n",
      "12 0 loss:  0.04204636812210083\n",
      "12 100 loss:  0.1051652804017067\n",
      "12 200 loss:  0.08389709889888763\n",
      "12 300 loss:  0.11051592230796814\n",
      "12  acc:  0.7709\n",
      "13 0 loss:  0.0585186704993248\n",
      "13 100 loss:  0.1519545614719391\n",
      "13 200 loss:  0.16522017121315002\n",
      "13 300 loss:  0.05145769938826561\n",
      "13  acc:  0.7502\n",
      "14 0 loss:  0.11357848346233368\n",
      "14 100 loss:  0.04997406154870987\n",
      "14 200 loss:  0.09516505897045135\n",
      "14 300 loss:  0.020908575505018234\n",
      "14  acc:  0.7621\n",
      "15 0 loss:  0.07273305207490921\n",
      "15 100 loss:  0.0461893156170845\n",
      "15 200 loss:  0.1273132860660553\n",
      "15 300 loss:  0.03115985356271267\n",
      "15  acc:  0.7713\n",
      "16 0 loss:  0.024958226829767227\n",
      "16 100 loss:  0.17852629721164703\n",
      "16 200 loss:  0.09094258397817612\n",
      "16 300 loss:  0.058273836970329285\n",
      "16  acc:  0.7617\n",
      "17 0 loss:  0.03088408336043358\n",
      "17 100 loss:  0.09186717867851257\n",
      "17 200 loss:  0.08221480995416641\n",
      "17 300 loss:  0.04645443335175514\n",
      "17  acc:  0.7647\n",
      "18 0 loss:  0.05151565372943878\n",
      "18 100 loss:  0.0324741005897522\n",
      "18 200 loss:  0.06159808859229088\n",
      "18 300 loss:  0.035386644303798676\n",
      "18  acc:  0.7701\n",
      "19 0 loss:  0.03223934397101402\n",
      "19 100 loss:  0.07901331782341003\n",
      "19 200 loss:  0.04367291182279587\n",
      "19 300 loss:  0.03358098492026329\n",
      "19  acc:  0.7501\n",
      "20 0 loss:  0.18846452236175537\n",
      "20 100 loss:  0.051264990121126175\n",
      "20 200 loss:  0.07855720072984695\n",
      "20 300 loss:  0.01931995525956154\n",
      "20  acc:  0.7617\n",
      "21 0 loss:  0.06901010870933533\n",
      "21 100 loss:  0.10314574837684631\n",
      "21 200 loss:  0.024577707052230835\n",
      "21 300 loss:  0.031956616789102554\n",
      "21  acc:  0.7706\n",
      "22 0 loss:  0.046952880918979645\n",
      "22 100 loss:  0.019958864897489548\n",
      "22 200 loss:  0.037161994725465775\n",
      "22 300 loss:  0.00464701559394598\n",
      "22  acc:  0.7749\n",
      "23 0 loss:  0.05330874025821686\n",
      "23 100 loss:  0.016487466171383858\n",
      "23 200 loss:  0.04574368894100189\n",
      "23 300 loss:  0.015851600095629692\n",
      "23  acc:  0.7681\n",
      "24 0 loss:  0.050057969987392426\n",
      "24 100 loss:  0.07412132620811462\n",
      "24 200 loss:  0.004149808082729578\n",
      "24 300 loss:  0.00847635418176651\n",
      "24  acc:  0.7779\n",
      "25 0 loss:  0.013926491141319275\n",
      "25 100 loss:  0.03732811659574509\n",
      "25 200 loss:  0.016996413469314575\n",
      "25 300 loss:  0.01867305487394333\n",
      "25  acc:  0.7654\n",
      "26 0 loss:  0.021956566721200943\n",
      "26 100 loss:  0.016095038503408432\n",
      "26 200 loss:  0.004643784370273352\n",
      "26 300 loss:  0.06737533211708069\n",
      "26  acc:  0.766\n",
      "27 0 loss:  0.028768522664904594\n",
      "27 100 loss:  0.03822997212409973\n",
      "27 200 loss:  0.023862335830926895\n",
      "27 300 loss:  0.07558013498783112\n",
      "27  acc:  0.7685\n",
      "28 0 loss:  0.011892443522810936\n",
      "28 100 loss:  0.023120161145925522\n",
      "28 200 loss:  0.030521247535943985\n",
      "28 300 loss:  0.02589082531630993\n",
      "28  acc:  0.7696\n",
      "29 0 loss:  0.030454155057668686\n",
      "29 100 loss:  0.019455838948488235\n",
      "29 200 loss:  0.0480152890086174\n",
      "29 300 loss:  0.007749446202069521\n",
      "29  acc:  0.7764\n",
      "30 0 loss:  0.01055880542844534\n",
      "30 100 loss:  0.04166720062494278\n",
      "30 200 loss:  0.033416859805583954\n",
      "30 300 loss:  0.005327199585735798\n",
      "30  acc:  0.773\n",
      "31 0 loss:  0.05923940986394882\n",
      "31 100 loss:  0.0246213860809803\n",
      "31 200 loss:  0.007807888090610504\n",
      "31 300 loss:  0.04574351757764816\n",
      "31  acc:  0.7784\n",
      "32 0 loss:  0.0053882417269051075\n",
      "32 100 loss:  0.13295379281044006\n",
      "32 200 loss:  0.04692301154136658\n",
      "32 300 loss:  0.04478804022073746\n",
      "32  acc:  0.7802\n",
      "33 0 loss:  0.0010478915646672249\n",
      "33 100 loss:  0.061364270746707916\n",
      "33 200 loss:  0.039768751710653305\n",
      "33 300 loss:  0.022274993360042572\n",
      "33  acc:  0.7772\n",
      "34 0 loss:  0.013964283280074596\n",
      "34 100 loss:  0.008227359503507614\n",
      "34 200 loss:  0.027593910694122314\n",
      "34 300 loss:  0.014344110153615475\n",
      "34  acc:  0.7782\n",
      "35 0 loss:  0.019965406507253647\n",
      "35 100 loss:  0.01477757841348648\n",
      "35 200 loss:  0.016911577433347702\n",
      "35 300 loss:  0.031090974807739258\n",
      "35  acc:  0.7761\n",
      "36 0 loss:  0.003029859159141779\n",
      "36 100 loss:  0.02240040898323059\n",
      "36 200 loss:  0.0291111022233963\n",
      "36 300 loss:  0.006959378719329834\n",
      "36  acc:  0.765\n",
      "37 0 loss:  0.01835092157125473\n",
      "37 100 loss:  0.005768053699284792\n",
      "37 200 loss:  0.0025474433787167072\n",
      "37 300 loss:  0.003973327577114105\n",
      "37  acc:  0.7788\n",
      "38 0 loss:  0.003923396579921246\n",
      "38 100 loss:  0.019499104470014572\n",
      "38 200 loss:  0.002718402538448572\n",
      "38 300 loss:  0.009386250749230385\n",
      "38  acc:  0.7702\n",
      "39 0 loss:  0.006575297098606825\n",
      "39 100 loss:  0.0053875986486673355\n",
      "39 200 loss:  0.01911258138716221\n",
      "39 300 loss:  0.07180193066596985\n",
      "39  acc:  0.782\n",
      "40 0 loss:  0.009180901572108269\n",
      "40 100 loss:  0.004190412349998951\n",
      "40 200 loss:  0.005284856539219618\n",
      "40 300 loss:  0.003854679875075817\n",
      "40  acc:  0.781\n",
      "41 0 loss:  0.006020549684762955\n",
      "41 100 loss:  0.036537352949380875\n",
      "41 200 loss:  0.015627827495336533\n",
      "41 300 loss:  0.0056335097178816795\n",
      "41  acc:  0.7819\n",
      "42 0 loss:  0.044648852199316025\n",
      "42 100 loss:  0.02553427591919899\n",
      "42 200 loss:  0.020171325653791428\n",
      "42 300 loss:  0.010334905236959457\n",
      "42  acc:  0.7772\n",
      "43 0 loss:  0.003612533677369356\n",
      "43 100 loss:  0.012607667595148087\n",
      "43 200 loss:  0.05027056485414505\n",
      "43 300 loss:  0.06542322784662247\n",
      "43  acc:  0.7834\n",
      "44 0 loss:  0.016174837946891785\n",
      "44 100 loss:  0.016598282381892204\n",
      "44 200 loss:  0.008672578260302544\n",
      "44 300 loss:  0.010690039955079556\n",
      "44  acc:  0.7799\n",
      "45 0 loss:  0.004515755455940962\n",
      "45 100 loss:  0.008332890458405018\n",
      "45 200 loss:  0.02841746062040329\n",
      "45 300 loss:  0.017038919031620026\n",
      "45  acc:  0.7816\n",
      "46 0 loss:  0.007499525789171457\n",
      "46 100 loss:  0.04353281110525131\n",
      "46 200 loss:  0.014017460867762566\n",
      "46 300 loss:  0.019345436245203018\n",
      "46  acc:  0.7892\n",
      "47 0 loss:  0.0028519139159470797\n",
      "47 100 loss:  0.006354959215968847\n",
      "47 200 loss:  0.012921575456857681\n",
      "47 300 loss:  0.043532274663448334\n",
      "47  acc:  0.7821\n",
      "48 0 loss:  0.006135511212050915\n",
      "48 100 loss:  0.011151556856930256\n",
      "48 200 loss:  0.02768215723335743\n",
      "48 300 loss:  0.02363109402358532\n",
      "48  acc:  0.777\n",
      "49 0 loss:  0.015526493079960346\n",
      "49 100 loss:  0.0557723268866539\n",
      "49 200 loss:  0.006746095605194569\n",
      "49 300 loss:  0.02706947550177574\n",
      "49  acc:  0.7804\n"
     ]
    }
   ],
   "source": [
    "conv_layers = [ # 5 units of conv + max pooling\n",
    "    # unit 1\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "    \n",
    "    # unit 2\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 3\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 4\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 5\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "]\n",
    "\n",
    "def main():\n",
    "    # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "    conv_net = Sequential(conv_layers)\n",
    "\n",
    "    fc_net = Sequential([\n",
    "        layers.Dense(256, activation=tf.nn.relu),\n",
    "        layers.Dense(128, activation=tf.nn.relu),\n",
    "        layers.Dense(10, activation=None),\n",
    "    ])\n",
    "\n",
    "    conv_net.build(input_shape=[None, 32, 32, 3])\n",
    "    fc_net.build(input_shape=[None, 512])\n",
    "    conv_net.summary()\n",
    "    fc_net.summary()\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    # [1, 2] + [3, 4] => [1, 2, 3, 4]\n",
    "    variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for step, (x, y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "                out = conv_net(x)\n",
    "                # flatten, => [b, 512]\n",
    "                out = tf.reshape(out, [-1, 512])\n",
    "                # [b, 512] => [b, 10]\n",
    "                logits = fc_net(out)\n",
    "                # [b] => [b, 10]\n",
    "                y_onehot = tf.one_hot(y, depth=10)\n",
    "                # compute loss\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables)) \n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, \"loss: \", float(loss))\n",
    "        \n",
    "        # test\n",
    "        total_num = 0\n",
    "        total_correct = 0\n",
    "        for x, y in test_db:\n",
    "            out = conv_net(x)\n",
    "            out = tf.reshape(out, [-1, 512])\n",
    "            logits = fc_net(out)\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "            correct = tf.reduce_sum(correct)\n",
    "\n",
    "            total_num += x.shape[0]\n",
    "            total_correct += int(correct)\n",
    "        \n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, \" acc: \", acc) \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分离卷积 (Depth-wise Separable Convolution)\n",
    "\n",
    "卷积核的每个通道与输入的每个通道做卷积运算，得到多个通道的中间特征，然后这些特征与多个1x1卷积核进行普通卷积运算，这些输出在通道轴上进行拼接\n",
    "优点：相同的输入输出，可以减少参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
