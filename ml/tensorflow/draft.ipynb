{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正则化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(14.03604, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.random.normal((4, 3))\n",
    "w2 = tf.random.normal((4, 2))\n",
    "\n",
    "loss_reg = tf.reduce_sum(tf.abs(w1)) + tf.reduce_sum(tf.abs(w2))\n",
    "print(loss_reg) # 正则化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 卷积运算\n",
    "  \n",
    "  感受野与卷积核**逐位相乘**后累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1:  (2, 3, 3, 4)\n",
      "out2:  (2, 5, 5, 4)\n",
      "out3:  (2, 3, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# 输入格式为：batch_shape + [in_height, in_width, in_channels]\n",
    "x = tf.random.normal([2, 5, 5, 3]) # 模拟输入，3通道，高宽为5\n",
    "# 创建4个3x3的卷积核，格式为[filter_height, filter_width, in_channels, out_channels]\n",
    "w = tf.random.normal([3, 3, 3, 4]) \n",
    "\n",
    "## 1. padding为0，步长为1\n",
    "out1 = tf.nn.conv2d(input=x, filters=w, strides=1, padding=[[0, 0], [0, 0], [0, 0], [0, 0]])\n",
    "print(\"out1: \", out1.shape)\n",
    "\n",
    "## 2. 上下左右各填充1个单位，步长为1\n",
    "#    when data_format is `\"NHWC\"`, padding should be in the form \n",
    "#           `[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]`. \n",
    "#    When explicit padding used and data_format is `\"NCHW\"`, this should be in the form \n",
    "#           `[[0, 0], [0, 0], [pad_top, pad_bottom], [pad_left, pad_right]]`.\n",
    "out2 = tf.nn.conv2d(input=x, filters=w, strides=1, padding=[[0, 0], [1, 1], [1, 1], [0, 0]])\n",
    "print(\"out2: \", out2.shape)\n",
    "\n",
    "## 3. padding=\"SAME\", 输出维度的高宽成1/s倍减少\n",
    "out3 = tf.nn.conv2d(input=x, filters=w, strides=2, padding=\"SAME\")\n",
    "print(\"out3: \", out3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 卷积层类\n",
    "\n",
    "`Conv2D`: 卷积层类，里面保存了张量W和偏置b，可以通过`trainable_variables`、`kernel`、`bias`等成员获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 创建4个3x3大小的卷积核层\n",
    "layer1 = tf.keras.layers.Conv2D(4, kernel_size=3, strides=1, padding=\"SAME\")\n",
    "\n",
    "## 2. 创建4个高x宽为3x4大小的卷积核，高宽方向的步长为2和1\n",
    "layer2 = tf.keras.layers.Conv2D(4, kernel_size=(3, 4), strides=(2, 1), padding=\"SAME\")\n",
    "\n",
    "out = layer1(x) # 前向计算\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 池化层\n",
    "\n",
    "从局部相关的一组元素中采样或进行信息聚合，从而得到新的元素值\n",
    "\n",
    "最大池化（Max Pooling）：最大值\n",
    "平均池化（Average Pooling）：平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BatchNormal层\n",
    "  \n",
    "  测试模式与训练模式要区分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "# 2 images with 4x4 size, 3 channels\n",
    "x = tf.random.normal([2, 4, 4, 3], mean=.1, stddev=0.5)\n",
    "\n",
    "net = layers.BatchNormalization(axis=-1, center=True, scale=True, trainable=True)\n",
    "\n",
    "out = net(x)\n",
    "print(\"forward in test model: \", net.variables)\n",
    "\n",
    "out = net(x, training=True)\n",
    "print(\"forward in train model(1 step): \", net.variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    out = net(x, training=True)\n",
    "print('forward in train mode(100 steps):', net.variables)\n",
    "\n",
    "\n",
    "optimizer = optimizers.SGD(lr=1e-2)\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = net(x, training=True)\n",
    "        loss = tf.reduce_mean(tf.pow(out, 2)) - 1\n",
    "\n",
    "    grads = tape.gradient(loss, net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "print('backward(10 steps):', net.variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CIFAR10训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 589s 3us/step\n",
      "170508288/170498071 [==============================] - 589s 3us/step\n",
      "(50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, optimizers, datasets, Sequential\n",
    "import os \n",
    "\n",
    "(x, y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    # [0, 1]\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 8, 8, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 4, 4, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 2, 2, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 1, 1, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,404,992\n",
      "Trainable params: 9,404,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165,514\n",
      "Trainable params: 165,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeting/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss:  2.302311420440674\n",
      "0 100 loss:  1.858573079109192\n",
      "0 200 loss:  1.6415003538131714\n",
      "0 300 loss:  1.5702300071716309\n",
      "1 0 loss:  1.3746676445007324\n",
      "1 100 loss:  1.34897780418396\n",
      "1 200 loss:  1.217454433441162\n",
      "1 300 loss:  1.1521364450454712\n",
      "2 0 loss:  1.2674027681350708\n",
      "2 100 loss:  1.1320360898971558\n",
      "2 200 loss:  1.2114136219024658\n",
      "2 300 loss:  1.1245837211608887\n",
      "3 0 loss:  1.0407662391662598\n",
      "3 100 loss:  0.9748767614364624\n",
      "3 200 loss:  0.8639795184135437\n",
      "3 300 loss:  1.0191333293914795\n",
      "4 0 loss:  1.1034660339355469\n",
      "4 100 loss:  0.9290702939033508\n",
      "4 200 loss:  0.8547897338867188\n",
      "4 300 loss:  0.7108937501907349\n",
      "5 0 loss:  0.6602210998535156\n",
      "5 100 loss:  0.8034095168113708\n",
      "5 200 loss:  0.5417741537094116\n",
      "5 300 loss:  0.7126283049583435\n",
      "6 0 loss:  0.6102094650268555\n",
      "6 100 loss:  0.6869699954986572\n",
      "6 200 loss:  0.5363408327102661\n",
      "6 300 loss:  0.6451226472854614\n",
      "7 0 loss:  0.6249597072601318\n",
      "7 100 loss:  0.5408921837806702\n",
      "7 200 loss:  0.5649421215057373\n",
      "7 300 loss:  0.42281925678253174\n",
      "8 0 loss:  0.5888094902038574\n",
      "8 100 loss:  0.5630884170532227\n",
      "8 200 loss:  0.543572187423706\n",
      "8 300 loss:  0.48540568351745605\n",
      "9 0 loss:  0.4660126566886902\n",
      "9 100 loss:  0.45288515090942383\n",
      "9 200 loss:  0.2909190356731415\n",
      "9 300 loss:  0.27048641443252563\n",
      "10 0 loss:  0.3039783835411072\n",
      "10 100 loss:  0.25029999017715454\n",
      "10 200 loss:  0.34405338764190674\n",
      "10 300 loss:  0.25255775451660156\n",
      "11 0 loss:  0.21528421342372894\n",
      "11 100 loss:  0.39005452394485474\n",
      "11 200 loss:  0.2409326434135437\n",
      "11 300 loss:  0.1770545095205307\n",
      "12 0 loss:  0.3041433095932007\n",
      "12 100 loss:  0.17340537905693054\n",
      "12 200 loss:  0.16949844360351562\n",
      "12 300 loss:  0.1305912882089615\n",
      "13 0 loss:  0.12276298552751541\n",
      "13 100 loss:  0.06681124120950699\n",
      "13 200 loss:  0.2521831691265106\n",
      "13 300 loss:  0.028969861567020416\n",
      "14 0 loss:  0.12460041791200638\n",
      "14 100 loss:  0.041493963450193405\n",
      "14 200 loss:  0.08545930683612823\n",
      "14 300 loss:  0.05523047596216202\n",
      "15 0 loss:  0.09121329337358475\n",
      "15 100 loss:  0.05933889001607895\n",
      "15 200 loss:  0.04940961301326752\n",
      "15 300 loss:  0.044421080499887466\n",
      "16 0 loss:  0.03889179974794388\n",
      "16 100 loss:  0.03248396888375282\n",
      "16 200 loss:  0.06911487132310867\n",
      "16 300 loss:  0.055048584938049316\n",
      "17 0 loss:  0.0664396733045578\n",
      "17 100 loss:  0.0837581679224968\n",
      "17 200 loss:  0.07589483261108398\n",
      "17 300 loss:  0.02789124846458435\n",
      "18 0 loss:  0.08180847764015198\n",
      "18 100 loss:  0.030649464577436447\n",
      "18 200 loss:  0.04808815196156502\n",
      "18 300 loss:  0.04590253159403801\n",
      "19 0 loss:  0.0565146803855896\n",
      "19 100 loss:  0.020057938992977142\n",
      "19 200 loss:  0.027390342205762863\n",
      "19 300 loss:  0.01658528484404087\n",
      "20 0 loss:  0.02477552369236946\n",
      "20 100 loss:  0.040110550820827484\n",
      "20 200 loss:  0.017087619751691818\n",
      "20 300 loss:  0.026543397456407547\n",
      "21 0 loss:  0.03133321553468704\n",
      "21 100 loss:  0.03154926002025604\n",
      "21 200 loss:  0.01870882697403431\n",
      "21 300 loss:  0.015437090769410133\n",
      "22 0 loss:  0.02136414125561714\n",
      "22 100 loss:  0.02788669988512993\n",
      "22 200 loss:  0.13212049007415771\n",
      "22 300 loss:  0.11352691054344177\n",
      "23 0 loss:  0.04559296742081642\n",
      "23 100 loss:  0.007872868329286575\n",
      "23 200 loss:  0.037783682346343994\n",
      "23 300 loss:  0.01642533205449581\n",
      "24 0 loss:  0.05535464733839035\n",
      "24 100 loss:  0.015628386288881302\n",
      "24 200 loss:  0.06096091866493225\n",
      "24 300 loss:  0.03300213813781738\n",
      "25 0 loss:  0.033479150384664536\n",
      "25 100 loss:  0.0912856012582779\n",
      "25 200 loss:  0.06275279819965363\n",
      "25 300 loss:  0.00792754627764225\n",
      "26 0 loss:  0.044876325875520706\n",
      "26 100 loss:  0.036883316934108734\n",
      "26 200 loss:  0.022691411897540092\n",
      "26 300 loss:  0.04437800124287605\n",
      "27 0 loss:  0.062018394470214844\n",
      "27 100 loss:  0.01905817538499832\n",
      "27 200 loss:  0.026956506073474884\n",
      "27 300 loss:  0.013493482023477554\n",
      "28 0 loss:  0.006339443847537041\n",
      "28 100 loss:  0.03820061311125755\n",
      "28 200 loss:  0.02590120956301689\n",
      "28 300 loss:  0.0039059575647115707\n",
      "29 0 loss:  0.041247591376304626\n",
      "29 100 loss:  0.066407710313797\n",
      "29 200 loss:  0.015324210748076439\n",
      "29 300 loss:  0.012993698008358479\n",
      "30 0 loss:  0.026353422552347183\n",
      "30 100 loss:  0.01695096120238304\n",
      "30 200 loss:  0.004695144481956959\n",
      "30 300 loss:  0.03701314702630043\n",
      "31 0 loss:  0.06227133423089981\n",
      "31 100 loss:  0.04075315594673157\n",
      "31 200 loss:  0.06879431009292603\n",
      "31 300 loss:  0.023203525692224503\n",
      "32 0 loss:  0.05766036733984947\n",
      "32 100 loss:  0.006862853653728962\n",
      "32 200 loss:  0.02266285941004753\n",
      "32 300 loss:  0.008760175667703152\n",
      "33 0 loss:  0.012845125049352646\n",
      "33 100 loss:  0.035034872591495514\n",
      "33 200 loss:  0.0028546799439936876\n",
      "33 300 loss:  0.007085777353495359\n",
      "34 0 loss:  0.034746043384075165\n",
      "34 100 loss:  0.04102093353867531\n",
      "34 200 loss:  0.011473504826426506\n",
      "34 300 loss:  0.0040534245781600475\n",
      "35 0 loss:  0.0019594032783061266\n",
      "35 100 loss:  0.02710583060979843\n",
      "35 200 loss:  0.007786529138684273\n",
      "35 300 loss:  0.06782332062721252\n",
      "36 0 loss:  0.019883684813976288\n",
      "36 100 loss:  0.017970586195588112\n",
      "36 200 loss:  0.04857444763183594\n",
      "36 300 loss:  0.03802315890789032\n",
      "37 0 loss:  0.016753392294049263\n",
      "37 100 loss:  0.019227907061576843\n",
      "37 200 loss:  0.028314895927906036\n",
      "37 300 loss:  0.00591639569029212\n",
      "38 0 loss:  0.005349097307771444\n",
      "38 100 loss:  0.016731683164834976\n",
      "38 200 loss:  0.007995007559657097\n",
      "38 300 loss:  0.01983952336013317\n",
      "39 0 loss:  0.003309000749140978\n",
      "39 100 loss:  0.028275396674871445\n",
      "39 200 loss:  0.025723004713654518\n",
      "39 300 loss:  0.01765846088528633\n",
      "40 0 loss:  0.02697320282459259\n",
      "40 100 loss:  0.005452477373182774\n",
      "40 200 loss:  0.016169702634215355\n",
      "40 300 loss:  0.023765534162521362\n",
      "41 0 loss:  0.049687355756759644\n",
      "41 100 loss:  0.011394312605261803\n",
      "41 200 loss:  0.0034144739620387554\n",
      "41 300 loss:  0.030067114159464836\n",
      "42 0 loss:  0.022209618240594864\n",
      "42 100 loss:  0.003664963413029909\n",
      "42 200 loss:  0.008021763525903225\n",
      "42 300 loss:  0.003315277397632599\n",
      "43 0 loss:  0.002614751923829317\n",
      "43 100 loss:  0.004403133410960436\n",
      "43 200 loss:  0.011404339224100113\n",
      "43 300 loss:  0.0035668741911649704\n",
      "44 0 loss:  0.002500369446352124\n",
      "44 100 loss:  0.017728496342897415\n",
      "44 200 loss:  0.00875205360352993\n",
      "44 300 loss:  0.02674091048538685\n",
      "45 0 loss:  0.008670452982187271\n",
      "45 100 loss:  0.008669944480061531\n",
      "45 200 loss:  0.0214926116168499\n",
      "45 300 loss:  0.00782361812889576\n",
      "46 0 loss:  0.007312294095754623\n",
      "46 100 loss:  0.04706697165966034\n",
      "46 200 loss:  0.03141875937581062\n",
      "46 300 loss:  0.02375842072069645\n",
      "47 0 loss:  0.018452893942594528\n",
      "47 100 loss:  0.06636981666088104\n",
      "47 200 loss:  0.004687197040766478\n",
      "47 300 loss:  0.006159250624477863\n",
      "48 0 loss:  0.0014878034126013517\n",
      "48 100 loss:  0.023314593359827995\n",
      "48 200 loss:  0.009409984573721886\n",
      "48 300 loss:  0.0022220583632588387\n",
      "49 0 loss:  0.07982202619314194\n",
      "49 100 loss:  0.03680688142776489\n",
      "49 200 loss:  0.019078433513641357\n",
      "49 300 loss:  0.031111983582377434\n"
     ]
    }
   ],
   "source": [
    "conv_layers = [ # 5 units of conv + max pooling\n",
    "    # unit 1\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "    \n",
    "    # unit 2\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 3\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 4\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 5\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "]\n",
    "\n",
    "def main():\n",
    "    # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "    conv_net = Sequential(conv_layers)\n",
    "\n",
    "    fc_net = Sequential([\n",
    "        layers.Dense(256, activation=tf.nn.relu),\n",
    "        layers.Dense(128, activation=tf.nn.relu),\n",
    "        layers.Dense(10, activation=None),\n",
    "    ])\n",
    "\n",
    "    conv_net.build(input_shape=[None, 32, 32, 3])\n",
    "    fc_net.build(input_shape=[None, 512])\n",
    "    conv_net.summary()\n",
    "    fc_net.summary()\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for step, (x, y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "                out = conv_net(x)\n",
    "                # flatten, => [b, 512]\n",
    "                out = tf.reshape(out, [-1, 512])\n",
    "                # [b, 512] => [b, 10]\n",
    "                logits = fc_net(out)\n",
    "                # [b] => [b, 10]\n",
    "                y_onehot = tf.one_hot(y, depth=10)\n",
    "                # compute loss\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables)) \n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, \"loss: \", float(loss))\n",
    "                \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
