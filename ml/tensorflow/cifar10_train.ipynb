{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential\n",
    "import os\n",
    "\n",
    "(x, y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    # [0, 1]\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(128)\n",
    "\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [  # 5 units of conv + max pooling\n",
    "    # unit 1\n",
    "    layers.Conv2D(64, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 2\n",
    "    layers.Conv2D(128, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 3\n",
    "    layers.Conv2D(256, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 4\n",
    "    layers.Conv2D(512, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "\n",
    "    # unit 5\n",
    "    layers.Conv2D(512, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3],\n",
    "                  padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "    conv_net = Sequential(conv_layers)\n",
    "\n",
    "    fc_net = Sequential([\n",
    "        layers.Dense(256, activation=tf.nn.relu),\n",
    "        layers.Dense(128, activation=tf.nn.relu),\n",
    "        layers.Dense(10, activation=None),\n",
    "    ])\n",
    "\n",
    "    conv_net.build(input_shape=[None, 32, 32, 3])\n",
    "    fc_net.build(input_shape=[None, 512])\n",
    "    conv_net.summary()\n",
    "    fc_net.summary()\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=1e-4)\n",
    "\n",
    "    # [1, 2] + [3, 4] => [1, 2, 3, 4]\n",
    "    variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for step, (x, y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "                out = conv_net(x)\n",
    "                # flatten, => [b, 512]\n",
    "                out = tf.reshape(out, [-1, 512])\n",
    "                # [b, 512] => [b, 10]\n",
    "                logits = fc_net(out)\n",
    "                # [b] => [b, 10]\n",
    "                y_onehot = tf.one_hot(y, depth=10)\n",
    "                # compute loss\n",
    "                loss = tf.losses.categorical_crossentropy(\n",
    "                    y_onehot, logits, from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, \"loss: \", float(loss))\n",
    "\n",
    "        # test\n",
    "        total_num = 0\n",
    "        total_correct = 0\n",
    "        for x, y in test_db:\n",
    "            out = conv_net(x)\n",
    "            out = tf.reshape(out, [-1, 512])\n",
    "            logits = fc_net(out)\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "            correct = tf.reduce_sum(correct)\n",
    "\n",
    "            total_num += x.shape[0]\n",
    "            total_correct += int(correct)\n",
    "\n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, \" acc: \", acc)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
